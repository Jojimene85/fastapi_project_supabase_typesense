# Lakehouse (pandas) â€“ EU Research Projects

> Proyecto de prueba tÃ©cnica que implementa una arquitectura tipo **Lakehouse** con **pandas + Parquet**, **Supabase (Postgres)** como DWH, **Typesense** para bÃºsqueda y **FastAPI** como API.  
> Incluye **Airflow standalone** (webserver + scheduler + sqlite en 1 contenedor) con **sensor** que dispara el pipeline al actualizarse un archivo en Bronze.

---

## ğŸŒ Contexto de los datos

Los datasets provienen de **proyectos de investigaciÃ³n cientÃ­fica en Europa** (ej. programas Horizon 2020, FP7).  
Incluyen informaciÃ³n de:

- **Proyectos**: tÃ­tulo, abstract, paÃ­s, programa, fechas, financiaciÃ³n.
- **Organizaciones**: entidades participantes, sector, paÃ­s.
- **Temas (topics)**: taxonomÃ­a cientÃ­fica asignada.
- **Prioridades de polÃ­tica**: alineaciÃ³n con ejes de la UE.
- **Base legal**: marco jurÃ­dico que sustenta el proyecto.
- **Vocabularios cientÃ­ficos (euroSciVoc)**: clasificaciones temÃ¡ticas.
- **Web items y links**: recursos digitales asociados.

Estos archivos alimentan la **capa Bronze**, y el pipeline construye una vista **star schema** en Gold con dimensiones y un hecho de financiaciÃ³n.

---

## ğŸ§­ Arquitectura

- **Data Lake local (filesystem)**  
  - `data/bronze/` â†’ archivos crudos (CSV)  
  - `data/silver/` â†’ datos limpios/normalizados en **Parquet**  
  - `data/gold/` â†’ **esquema estrella** en **Parquet**

- **ETL**: Python + pandas  
  - `bronze_to_silver.py` â†’ limpieza/normalizaciÃ³n  
  - `silver_to_gold.py` â†’ modelado en esquema estrella  
  - `sync_to_supabase.py` â†’ carga en Postgres (Supabase)

- **OrquestaciÃ³n**:  
  - `orchestration/run_etl.py` â†’ **SSOT** con `run_all()`  
  - Airflow (`lakehouse_watch_fixed_file.py`) â†’ sensor que vigila cambios en Bronze

- **API (FastAPI)**  
  - `/seed` â†’ dispara el pipeline completo  
  - `/raw` â†’ CRUD de archivos en Bronze  
  - `/gold` â†’ queries sobre tablas en Supabase  
  - `/search` â†’ bÃºsqueda en Typesense

- **Search**: Typesense  
  - Indexa proyectos (dim_project) para bÃºsqueda de texto y facetas por paÃ­s/aÃ±o.

---

## ğŸ“‚ Archivos y Carpetas

```
.
â”œâ”€ api/                        # API FastAPI
â”‚  â”œâ”€ main.py                  # Arranca la API y registra routers
â”‚  â”œâ”€ auth.py                  # Basic Auth (usuario/contraseÃ±a)
â”‚  â”œâ”€ routes/                  # Endpoints
â”‚  â”‚  â”œâ”€ seed.py               # /seed â†’ corre run_all()
â”‚  â”‚  â”œâ”€ raw.py                # /raw â†’ CRUD en Bronze
â”‚  â”‚  â”œâ”€ gold.py               # /gold â†’ queries a Supabase
â”‚  â”‚  â””â”€ search.py             # /search â†’ init, index, query en Typesense
â”‚  â”œâ”€ services/                # Conexiones externas
â”‚  â”‚  â”œâ”€ db.py                 # ConexiÃ³n SQLAlchemy a Supabase/Postgres
â”‚  â”‚  â””â”€ typesense_client.py   # Cliente Typesense
â”‚  â””â”€ domain/                  # Modelos Pydantic
â”‚     â”œâ”€ raw_models.py
â”‚     â””â”€ gold_models.py
â”‚
â”œâ”€ etl/                        # ETLs en pandas
â”‚  â”œâ”€ bronze_to_silver.py      # CSV â†’ Parquet (Silver)
â”‚  â”œâ”€ silver_to_gold.py        # Silver â†’ esquema estrella (Gold)
â”‚  â””â”€ sync_to_supabase.py      # Gold â†’ Supabase (truncate & load)
â”‚
â”œâ”€ orchestration/
â”‚  â””â”€ run_etl.py               # FunciÃ³n run_all(), usada por /seed y DAG
â”‚
â”œâ”€ dags/
â”‚  â””â”€ lakehouse_watch_fixed_file.py # DAG con sensor por archivo fijo
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ bronze/                  # Archivos crudos
â”‚  â”œâ”€ silver/                  # Parquet normalizados
â”‚  â””â”€ gold/                    # Parquet star schema
â”‚
â”œâ”€ logs/, plugins/             # Carpetas usadas por Airflow
â”œâ”€ .vol/                       # Persistencia (Typesense, etc.)
â”‚
â”œâ”€ docker-compose.yml          # Orquesta API, Typesense y Airflow
â”œâ”€ Dockerfile                  # Imagen de la API
â”œâ”€ requirements.txt            # Dependencias Python
â”œâ”€ .dockerignore               # Exclusiones para Docker build
â”œâ”€ Makefile                    # Atajos de comandos (make up, make seed, etc.)
â””â”€ README.md                   # Este archivo
```

---

## ğŸš€ Puesta en marcha (simple)

1. Crear carpetas necesarias:
   ```bash
   mkdir -p dags orchestration etl data/bronze data/silver data/gold logs plugins .vol/typesense
   ```

2. Colocar CSVs en `data/bronze/` (ej. `project.csv` con nombre fijo).

3. Levantar servicios:
   ```bash
   docker compose up -d --build
   ```

4. API:
   ```bash
   curl -u admin:supersecret -X POST http://localhost:8000/seed
   ```

5. Airflow UI: [http://localhost:8080](http://localhost:8080)  
   - Activar DAG `lakehouse_watch_fixed_file`.  
   - El DAG detecta cambios en `project.csv` y dispara full reload.

---

## âœ… Checklist

- [x] Bronze/Silver/Gold con pandas  
- [x] Supabase/Postgres como DWH  
- [x] Esquema estrella (dims + fact)  
- [x] API FastAPI con Basic Auth  
- [x] Typesense para bÃºsqueda facetada  
- [x] Airflow (standalone) con sensor de archivo fijo  

---

## ğŸ’¬ Nota final

Este proyecto simplifica la arquitectura para fines de demo, pero los patrones son **directamente migrables a Databricks**:  
- Parquet â†’ Delta Lake  
- pandas â†’ PySpark  
- Airflow â†’ Jobs/Workflows/DLT  
- Supabase â†’ SQL Warehouse  
